{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-d3d2e989-67f4-4f59-9f6e-ea40fe7312ea",
    "deepnote_cell_type": "code"
   },
   "source": "from torch import nn\nclass UNET(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n        self.conv2 = self.contract_block(32, 64, 3, 1)\n        self.conv3 = self.contract_block(64, 128, 3, 1)\n\n        self.upconv3 = self.expand_block(128, 64, 3, 1)\n        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n\n    def __call__(self, x):\n\n        # downsampling part\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n\n        upconv3 = self.upconv3(conv3)\n\n        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n\n        return upconv1\n\n    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n\n        contract = nn.Sequential(\n            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n            torch.nn.BatchNorm2d(out_channels),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n            torch.nn.BatchNorm2d(out_channels),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n                                 )\n\n        return contract\n\n    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n\n        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n                            torch.nn.BatchNorm2d(out_channels),\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n                            torch.nn.BatchNorm2d(out_channels),\n                            torch.nn.ReLU(),\n                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n                            )\n        return expand",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "unet = UNET(4,2)",
   "metadata": {
    "tags": [],
    "cell_id": "00001-43e9b2cb-af2a-4f18-b958-29f1cd289354",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# testing one pass\nxb, yb = next(iter(train_dl))\nxb.shape, yb.shape",
   "metadata": {
    "tags": [],
    "cell_id": "00002-648ed2db-0478-422b-807c-45bec9635212",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "pred = unet(xb)\npred.shape",
   "metadata": {
    "tags": [],
    "cell_id": "00003-f11f5908-6ae4-479b-888e-68a642747b7a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import time\nfrom IPython.display import clear_output\n\ndef train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n    start = time.time()\n    model.cuda()\n\n    train_loss, valid_loss = [], []\n\n    best_acc = 0.0\n\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch, epochs - 1))\n        print('-' * 10)\n\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train(True)  # Set trainind mode = true\n                dataloader = train_dl\n            else:\n                model.train(False)  # Set model to evaluate mode\n                dataloader = valid_dl\n\n            running_loss = 0.0\n            running_acc = 0.0\n\n            step = 0\n\n            # iterate over data\n            for x, y in dataloader:\n                x = x.cuda()\n                y = y.cuda()\n                step += 1\n\n                # forward pass\n                if phase == 'train':\n                    # zero the gradients\n                    optimizer.zero_grad()\n                    outputs = model(x)\n                    loss = loss_fn(outputs, y)\n\n                    # the backward pass frees the graph memory, so there is no \n                    # need for torch.no_grad in this training pass\n                    loss.backward()\n                    optimizer.step()\n                    # scheduler.step()\n\n                else:\n                    with torch.no_grad():\n                        outputs = model(x)\n                        loss = loss_fn(outputs, y.long())\n\n                # stats - whatever is the phase\n                acc = acc_fn(outputs, y)\n\n                running_acc  += acc*dataloader.batch_size\n                running_loss += loss*dataloader.batch_size \n\n                if step % 100 == 0:\n                    # clear_output(wait=True)\n                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n                    # print(torch.cuda.memory_summary())\n\n            epoch_loss = running_loss / len(dataloader.dataset)\n            epoch_acc = running_acc / len(dataloader.dataset)\n\n            clear_output(wait=True)\n            print('Epoch {}/{}'.format(epoch, epochs - 1))\n            print('-' * 10)\n            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n            print('-' * 10)\n\n            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n    \n    return train_loss, valid_loss    \n\ndef acc_metric(predb, yb):\n    return (predb.argmax(dim=1) == yb.cuda()).float().mean()",
   "metadata": {
    "tags": [],
    "cell_id": "00004-c769d971-23e3-4917-a10e-0602689ba44e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "loss_fn = nn.CrossEntropyLoss()\nopt = torch.optim.Adam(unet.parameters(), lr=0.01)\ntrain_loss, valid_loss = train(unet, train_dl, valid_dl, loss_fn, opt, acc_metric, epochs=50)",
   "metadata": {
    "tags": [],
    "cell_id": "00005-ee16a327-c4f2-4e3e-bc52-cfcbb5d0369d",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10,8))\nplt.plot(train_loss, label='Train loss')\nplt.plot(valid_loss, label='Valid loss')\nplt.legend()",
   "metadata": {
    "tags": [],
    "cell_id": "00006-e3947045-1e2f-4cfc-a9e4-39d0e337808f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(10,8))\nplt.plot(train_loss, label='Train loss')\nplt.plot(valid_loss, label='Valid loss')\nplt.legend()",
   "metadata": {
    "tags": [],
    "cell_id": "00007-3e1411c6-6469-4d1c-9831-a3b4dabda0e1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def batch_to_img(xb, idx):\n    img = np.array(xb[idx,0:3])\n    return img.transpose((1,2,0))\n\ndef predb_to_mask(predb, idx):\n    p = torch.functional.F.softmax(predb[idx], 0)\n    return p.argmax(0).cpu()",
   "metadata": {
    "tags": [],
    "cell_id": "00008-f2fd63f0-acb2-421e-982b-eaa553d3a6d2",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "xb, yb = next(iter(train_dl))\n\nwith torch.no_grad():\n    predb = unet(xb.cuda())\n\npredb.shape",
   "metadata": {
    "tags": [],
    "cell_id": "00009-173b7c0c-d102-430d-8e53-586b36cd0075",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "bs = 12\nfig, ax = plt.subplots(bs,3, figsize=(15,bs*5))\nfor i in range(bs):\n    ax[i,0].imshow(batch_to_img(xb,i))\n    ax[i,1].imshow(yb[i])\n    ax[i,2].imshow(predb_to_mask(predb, i))",
   "metadata": {
    "tags": [],
    "cell_id": "00010-ea05c670-e953-4cc9-a12d-dc3a1017f4f7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c4647907-5e58-403c-82b1-dade668d48a5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "324dc298-e1d5-4bff-a5d8-11c3a73607e4",
  "deepnote_execution_queue": []
 }
}